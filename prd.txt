Product Requirements Document: DAG Cost Tracker
1. Overview
Product Name: DAG Cost Tracker
Version: 1.0 (MVP)
Document Owner: Engineering Team
Last Updated: January 22, 2026
1.1 Executive Summary
DAG Cost Tracker is an observability and FinOps tool that provides cost visibility for data pipeline orchestration platforms. It tracks, estimates, and reports on the compute costs of individual DAG 
(Directed Acyclic Graph) runs, enabling teams to optimize their data pipeline spending.
1.2 Problem Statement
Organizations running production data pipelines lack granular cost visibility:

No DAG-level attribution: Airflow tracks execution metrics but not costs. Teams cannot answer "Which DAG cost us $10K last month?"
Manual cost analysis: Engineers spend hours correlating cloud bills with pipeline logs to identify expensive workflows
Reactive cost management: Teams discover cost overruns weeks later through cloud bills, not in real-time
No cost forecasting: No way to estimate costs before deploying new DAGs or modifications

Impact: Without DAG-level cost tracking, teams overspend on data infrastructure, miss optimization opportunities, and struggle to implement FinOps best practices.
1.3 Success Metrics

Adoption: 100+ DAG runs tracked within first month of deployment
Time savings: Reduce cost analysis time from 4 hours/week to 15 minutes/week
Cost optimization: Enable teams to identify and fix at least one cost anomaly per month
User satisfaction: 80% of users find cost reports actionable


2. User Personas
2.1 Primary: Data Platform Engineer (Maya)

Role: Maintains Airflow infrastructure for 50+ DAGs
Pain points: Gets alerts about high cloud bills but can't pinpoint which DAG caused the spike
Goals: Quickly identify expensive DAGs, set up cost guardrails, prove ROI of pipeline optimizations
Technical level: Expert in Airflow, comfortable with Python CLIs

2.2 Secondary: Analytics Engineer (James)

Role: Builds SQL-based ETL pipelines in dbt/Airflow
Pain points: Doesn't know if their DAG costs $5 or $500 per run
Goals: Estimate costs before deploying new pipelines, optimize expensive queries
Technical level: Strong SQL, basic Python knowledge

2.3 Tertiary: Engineering Manager (Priya)

Role: Owns budget for data platform team
Pain points: Receives $50K cloud bills with no breakdown by project or pipeline
Goals: Track cost trends, allocate spending to teams, justify infrastructure investments
Technical level: Non-technical, consumes reports/dashboards


3. Product Scope
3.1 In Scope (MVP)
Core tracking capabilities:

Airflow plugin that hooks into task lifecycle (on_task_start, on_task_end)
Capture execution time per task and DAG run
Store cost data in SQLite database (local to Airflow instance)

Cost calculation:

Support Snowflake warehouse pricing model (credits/hour)
Manual configuration of warehouse rates via config file
Formula: cost = execution_seconds / 3600 × warehouse_rate

Reporting:

CLI tool: dag-cost report --top 10 --period 7d
Show: DAG name, total runs, total cost, average cost per run, cost trend
Export to CSV

Configuration:

YAML config file for warehouse pricing
Example: SMALL_WH: {credits_per_hour: 1, cost_per_credit: 2.5}

3.2 Out of Scope (Future Versions)

Support for BigQuery, Databricks, EMR (post-MVP)
Slack/email alerts for cost thresholds
Web UI dashboard
Integration with Prefect, Dagster
Cloud-native storage (PostgreSQL, external APIs)
Automatic cost estimation before DAG execution
Cost allocation by team/project tags
Real-time cost monitoring

3.3 Non-Goals

Replace cloud provider billing dashboards
Track non-compute costs (storage, networking, data transfer)
Optimize DAGs automatically (we provide visibility, not auto-fixes)


4. Functional Requirements
4.1 Cost Tracking
FR-1.1: Task Execution Monitoring

The system SHALL capture start time, end time, and duration for every Airflow task execution
The system SHALL identify the warehouse/compute resource used by each task
Data captured: task_id, dag_id, run_id, start_time, end_time, duration_seconds, warehouse_name

FR-1.2: Cost Calculation

The system SHALL calculate cost using the formula: cost = (duration_seconds / 3600) × warehouse_credits_per_hour × cost_per_credit
The system SHALL support custom warehouse pricing via configuration
The system SHALL handle tasks with no warehouse assignment (cost = 0)

FR-1.3: Data Persistence

The system SHALL store cost records in a SQLite database
Schema: dag_runs table with columns: id, dag_id, run_id, execution_date, total_cost, duration_seconds, task_count, created_at
Schema: task_costs table with columns: id, dag_run_id, task_id, warehouse_name, duration_seconds, cost, start_time, end_time
The system SHALL retain cost data for at least 90 days

4.2 Configuration
FR-2.1: Warehouse Pricing Config

The system SHALL read warehouse pricing from a YAML config file at ~/.dag_cost_tracker/config.yaml
Config format:

yaml  warehouses:
    COMPUTE_WH_SMALL:
      credits_per_hour: 1
      cost_per_credit: 2.5
    COMPUTE_WH_LARGE:
      credits_per_hour: 4
      cost_per_credit: 2.5
```
- The system SHALL validate config on startup and show errors if invalid

**FR-2.2: Plugin Configuration**
- The system SHALL allow users to enable/disable cost tracking via Airflow config
- Airflow config: `[dag_cost_tracker] enabled = True`
- The system SHALL provide a default warehouse name if task doesn't specify one

### 4.3 Reporting

**FR-3.1: CLI Report Command**
- The system SHALL provide a command: `dag-cost report`
- Flags:
  - `--top N`: Show top N most expensive DAGs (default: 10)
  - `--period <days>d`: Time window in days (default: 7d)
  - `--format <table|csv>`: Output format (default: table)
  - `--dag-id <name>`: Filter to specific DAG
  - `--sort-by <cost|runs|avg_cost>`: Sort criteria (default: cost)

**FR-3.2: Report Contents**
- The report SHALL display:
  - DAG name
  - Total runs in period
  - Total cost
  - Average cost per run
  - Cost trend (% change vs. previous period)
- The report SHALL format costs with 2 decimal places and currency symbol
- The report SHALL sort results by total cost (descending) by default

**FR-3.3: Detailed DAG Report**
- The system SHALL provide a command: `dag-cost inspect <dag_id>`
- The report SHALL show:
  - Last 10 runs with individual costs
  - Most expensive tasks in the DAG
  - Cost breakdown by task
  - Historical cost trend (last 30 days)

### 4.4 Airflow Integration

**FR-4.1: Plugin Installation**
- The system SHALL be installable via pip: `pip install dag-cost-tracker`
- The system SHALL auto-register as an Airflow plugin when installed
- The system SHALL not require DAG code modifications

**FR-4.2: Task Hooks**
- The system SHALL use Airflow's task lifecycle hooks: `on_task_instance_running`, `on_task_instance_success`, `on_task_instance_failed`
- The system SHALL track costs for successful AND failed tasks
- The system SHALL extract warehouse information from task context or operator parameters

---

## 5. Non-Functional Requirements

### 5.1 Performance

- **NFR-1.1**: Cost tracking SHALL add <100ms overhead per task execution
- **NFR-1.2**: Database writes SHALL be asynchronous to avoid blocking task execution
- **NFR-1.3**: CLI reports SHALL return results in <5 seconds for 10K DAG runs

### 5.2 Reliability

- **NFR-2.1**: Plugin failures SHALL NOT cause Airflow task failures
- **NFR-2.2**: The system SHALL log errors to Airflow logs without raising exceptions
- **NFR-2.3**: The system SHALL gracefully handle missing warehouse pricing (assume cost = 0, log warning)

### 5.3 Scalability

- **NFR-3.1**: The system SHALL support tracking 1000+ DAG runs per day
- **NFR-3.2**: SQLite database SHALL remain performant up to 1M task cost records
- **NFR-3.3**: The system SHALL provide database cleanup utilities to archive old data

### 5.4 Usability

- **NFR-4.1**: Installation SHALL require ≤5 steps (install package, create config, enable plugin, restart Airflow)
- **NFR-4.2**: CLI output SHALL be human-readable with clear column headers
- **NFR-4.3**: The system SHALL provide example config files in documentation

### 5.5 Maintainability

- **NFR-5.1**: Code SHALL follow PEP 8 style guidelines
- **NFR-5.2**: The system SHALL have 80%+ test coverage
- **NFR-5.3**: The system SHALL use semantic versioning

---

## 6. User Stories

### 6.1 Cost Visibility

**US-1**: As a data engineer, I want to see which DAGs cost the most, so I can prioritize optimization efforts.

**Acceptance Criteria:**
- I can run `dag-cost report --top 10` and see a ranked list
- Report shows total cost and number of runs
- I can filter by time period (7d, 30d, 90d)

---

**US-2**: As an analytics engineer, I want to track costs for my specific DAG, so I can verify optimizations worked.

**Acceptance Criteria:**
- I can run `dag-cost inspect my_etl_dag`
- Report shows cost per run over the last 30 days
- I can see which tasks are most expensive

---

### 6.2 Cost Anomaly Detection

**US-3**: As a platform engineer, I want to identify when a DAG's cost suddenly increases, so I can investigate issues quickly.

**Acceptance Criteria:**
- Report shows % change in cost vs. previous period
- I can spot DAGs with >50% cost increases at a glance
- I can drill down to see which run caused the spike

---

### 6.3 Configuration

**US-4**: As a platform engineer, I want to configure warehouse pricing, so costs reflect my actual Snowflake bill.

**Acceptance Criteria:**
- I can create a YAML config with multiple warehouses
- I can set credits/hour and cost/credit for each warehouse
- System validates my config and shows clear error messages

---

### 6.4 Historical Analysis

**US-5**: As an engineering manager, I want to export cost data to CSV, so I can create executive reports.

**Acceptance Criteria:**
- I can run `dag-cost report --format csv > costs.csv`
- CSV includes all columns from the table report
- CSV is importable into Excel/Google Sheets

---

## 7. Technical Architecture

### 7.1 System Components
```
┌─────────────────────────────────────────────────┐
│             Airflow Scheduler                    │
│  ┌───────────────────────────────────────────┐  │
│  │      DAG Cost Tracker Plugin              │  │
│  │  ┌─────────────────────────────────────┐  │  │
│  │  │   Task Lifecycle Hooks              │  │  │
│  │  │   - on_task_instance_running        │  │  │
│  │  │   - on_task_instance_success        │  │  │
│  │  └─────────────────────────────────────┘  │  │
│  │  ┌─────────────────────────────────────┐  │  │
│  │  │   Cost Calculator                   │  │  │
│  │  │   - Load warehouse pricing          │  │  │
│  │  │   - Calculate task costs            │  │  │
│  │  └─────────────────────────────────────┘  │  │
│  │  ┌─────────────────────────────────────┐  │  │
│  │  │   Database Writer                   │  │  │
│  │  │   - Async writes to SQLite          │  │  │
│  │  └─────────────────────────────────────┘  │  │
│  └───────────────────────────────────────────┘  │
└─────────────────────────────────────────────────┘
                      │
                      ▼
         ┌─────────────────────────┐
         │   SQLite Database       │
         │   - dag_runs            │
         │   - task_costs          │
         └─────────────────────────┘
                      │
                      ▼
         ┌─────────────────────────┐
         │   CLI Tool              │
         │   - dag-cost report     │
         │   - dag-cost inspect    │
         └─────────────────────────┘
7.2 Data Models
dag_runs table:
sqlCREATE TABLE dag_runs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    dag_id TEXT NOT NULL,
    run_id TEXT NOT NULL,
    execution_date TIMESTAMP NOT NULL,
    total_cost REAL DEFAULT 0,
    duration_seconds INTEGER DEFAULT 0,
    task_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(dag_id, run_id)
);
CREATE INDEX idx_dag_runs_dag_id ON dag_runs(dag_id);
CREATE INDEX idx_dag_runs_execution_date ON dag_runs(execution_date);
task_costs table:
sqlCREATE TABLE task_costs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    dag_run_id INTEGER NOT NULL,
    task_id TEXT NOT NULL,
    warehouse_name TEXT,
    duration_seconds INTEGER NOT NULL,
    cost REAL NOT NULL,
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (dag_run_id) REFERENCES dag_runs(id)
);
CREATE INDEX idx_task_costs_dag_run_id ON task_costs(dag_run_id);
7.3 Technology Stack

Language: Python 3.8+
Framework: Apache Airflow 2.0+ plugin system
Database: SQLite 3
CLI: Click library
Testing: pytest
Packaging: setuptools, PyPI


8. Implementation Plan
Phase 1: Core Infrastructure (Weeks 1-2)
Deliverables:

SQLite database schema and migrations
Config file parser (YAML)
Cost calculation engine
Unit tests for core logic

Tasks:

Set up Python project structure
Implement database models with SQLAlchemy
Build warehouse pricing config loader
Write cost calculation function
Add comprehensive unit tests (80% coverage target)


Phase 2: Airflow Plugin (Weeks 3-4)
Deliverables:

Airflow plugin with task hooks
Async database writer
Integration tests with Airflow

Tasks:

Create Airflow plugin class
Implement task lifecycle hooks
Extract warehouse info from task context
Build async database writer with error handling
Test with sample Airflow DAGs


Phase 3: CLI Reporting (Week 5)
Deliverables:

dag-cost report command
dag-cost inspect command
CSV export functionality

Tasks:

Build CLI using Click framework
Implement report queries with SQL
Create formatted table output
Add CSV export
Write CLI integration tests


Phase 4: Documentation & Polish (Week 6)
Deliverables:

README with installation guide
Example config files
Troubleshooting guide
PyPI package

Tasks:

Write comprehensive README
Create example Airflow DAG with cost tracking
Document common issues and fixes
Package for PyPI distribution
Create demo video/screenshots


9. Success Criteria & Metrics
9.1 Launch Criteria
The MVP is ready to launch when:

✅ All functional requirements (FR-1 through FR-4) are implemented
✅ Test coverage ≥80%
✅ Installation works on fresh Airflow 2.x instance
✅ CLI reports run successfully with sample data
✅ Documentation covers installation, configuration, and usage

9.2 Success Metrics (30 days post-launch)
Adoption metrics:

3+ teams using the tool in production
500+ DAG runs tracked
10+ GitHub stars

Usage metrics:

Average 5 CLI report commands per user per week
At least 1 cost optimization identified per team

Quality metrics:

Zero critical bugs reported
<5% plugin overhead on task execution time
90% uptime (plugin doesn't crash Airflow)


10. Risks & Mitigations
10.1 Technical Risks
RiskImpactProbabilityMitigationPlugin crashes Airflow schedulerHighMediumComprehensive error handling, fail-safe design, extensive testingSQLite performance degrades with large 
datasetsMediumMediumAdd data retention policy, document PostgreSQL migration pathWarehouse cost extraction fails for custom operatorsMediumHighProvide manual warehouse tagging API, clear 
documentationIncorrect cost calculationsHighLowThorough unit tests, validation against real Snowflake bills
10.2 Adoption Risks
RiskImpactProbabilityMitigationUsers don't trust cost accuracyHighMediumShow calculation formula, provide validation guideInstallation too complexMediumMediumCreate one-command installer, provide 
Docker exampleTeams already use custom solutionsLowHighEmphasize ease of migration, offer import scripts

11. Open Questions

Warehouse detection: How do we reliably detect which Snowflake warehouse a task uses?

Resolution approach: Start with manual config per DAG, explore auto-detection in v2


Multi-warehouse DAGs: What if a single DAG uses multiple warehouses?

Resolution approach: Track costs per task, aggregate at DAG level


Cost accuracy: How do we validate our costs match actual cloud bills?

Resolution approach: Provide validation script that compares our totals with Snowflake query history


Concurrent DAG runs: How do we handle parallel executions of the same DAG?

Resolution approach: Use Airflow's run_id to uniquely identify each execution




12. Future Enhancements (Post-MVP)
v1.1: Alerting & Notifications

Slack webhooks for cost threshold alerts
Email digests for weekly cost reports
Anomaly detection using statistical methods

v1.2: Multi-Platform Support

BigQuery cost tracking
Databricks cluster cost tracking
EMR job cost tracking

v1.3: Advanced Reporting

Web UI dashboard (Flask/Streamlit)
Cost forecasting based on historical trends
Budget allocation and tracking

v1.4: Enterprise Features

Cost allocation by team/project tags
PostgreSQL backend for multi-Airflow deployments
Integration with Prefect and Dagster


13. Appendix
13.1 Snowflake Pricing Model
Snowflake charges are based on:

Warehouse size: XS to 6XL (1 to 128 credits/hour)
Credit cost: Typically $2-4 per credit depending on edition
Auto-suspend: Warehouses can pause when idle

Our cost formula: cost = (query_time_seconds / 3600) × warehouse_credits_per_hour × cost_per_credit
Example:

Task runs for 300 seconds (5 minutes)
Uses SMALL warehouse (1 credit/hour)
Credits cost $2.50 each
Cost = (300 / 3600) × 1 × 2.50 = $0.208

13.2 Competitive Landscape

Airflow native: Only tracks execution time, not costs
Cloud provider tools: Show overall spend, no DAG attribution
dbt Cloud: Has cost tracking for dbt models only
Monte Carlo, Datafold: Data observability tools, not cost-focused
CloudHealth, CloudCheckr: General cloud cost tools, no DAG-level granularity

Our differentiation: First tool purpose-built for DAG-level cost tracking with native Airflow integration.
13.3 Example Use Cases
Use Case 1: Cost Regression Detection

Team deploys new version of user_segmentation_dag
Next day, dag-cost report shows 300% cost increase
dag-cost inspect user_segmentation_dag reveals new aggregate_events task costs $50/run
Investigation finds missing date filter, query scans entire table
Fix deployed, cost returns to normal

Use Case 2: Budget Planning

Manager runs dag-cost report --period 30d --format csv
Exports to spreadsheet, calculates monthly run rate: $15K/month
Uses data to request budget increase for Q2
Sets up monthly cost review using the tool

Use Case 3: Warehouse Rightsizing

Report shows nightly_aggregation_dag costs $200/run
Team inspects task breakdown, finds 90% of cost in one task
Task runs on LARGE warehouse but completes in 2 minutes
Switch to SMALL warehouse, cost drops to $50/run with no performance impact
